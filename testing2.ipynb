{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b9fb3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anush/anaconda3/lib/python3.9/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_training' (0.0.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "from resume_parser import resumeparse\n",
    "from tika import parser  \n",
    "import locationtagger\n",
    "import re\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from pdfminer.high_level import extract_text\n",
    "import aspose.words as aw\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "from PyPDF2 import PdfReader\n",
    "import os\n",
    "import pathlib\n",
    "####################################################\n",
    "\n",
    "import glob\n",
    "from pdfminer.high_level import extract_text\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1944c907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/anush/Desktop/flask_name/new_pdf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_dir = pathlib.Path(__name__).parent.absolute()\n",
    "pdf_folder_path = os.path.join(base_dir,'new_pdf',)\n",
    "pdf_file_path  = os.path.join(base_dir,'new_pdf','*')\n",
    "pdf_folder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e1482f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_convert_to_pdf(path):\n",
    "#     count =0\n",
    "    cunt =0\n",
    "    cnt = 0\n",
    "    for file_ in glob.glob(path):\n",
    "        try:\n",
    "            if file_.endswith(\".doc\"):\n",
    "                print(file_,'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa')\n",
    "                doc = aw.Document(file_)\n",
    "                doc.save(os.path.join(base_dir,'Training_resume',f\"+{cunt}.pdf\"))\n",
    "                cunt+=1\n",
    "            elif file_.endswith(\".rtf\"):\n",
    "                print(file_,'bbbbbbbbbbbbbbbbbbbbbbbbbbbbbb')\n",
    "                doc = aw.Document(file_)\n",
    "                doc.save(os.path.join(base_dir,'Training_resume',f\"++{cunt}.pdf\"))\n",
    "                cnt+=1        \n",
    "        except  Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "#convert word to text\n",
    "def  extract_text_from_word(word_path):\n",
    "    doc = docx.Document(word_path)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "\n",
    "#convert pdf to text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "\n",
    "#return text\n",
    "def return_text(path):\n",
    "    if path.endswith('.docx'):\n",
    "        return extract_text_from_word(path)\n",
    "    elif path.endswith('.pdf'):\n",
    "        return extract_text_from_pdf(path)\n",
    "    \n",
    "\n",
    "#convert_the_info_dic\n",
    "def convert_the_info_dic(text):\n",
    "    #breaking the text\n",
    "    dit ={}\n",
    "    title = ['Education','Qualification','Skills','Objective','Experience','Tools','Designation','Employment']\n",
    "    for i in title:\n",
    "        text = text.replace(f'{i}',f'|{i}').replace(f'{i.upper()}',f'|{i.upper()}')\n",
    "    text = text.split(\"|\")\n",
    "\n",
    "    #making dictionary of inforamtion\n",
    "    for titl in title:\n",
    "        check = False\n",
    "        for txt in text: \n",
    "            if titl in txt or titl.upper() in txt:\n",
    "                check = True\n",
    "                dit[titl] = txt \n",
    "        if check == False:\n",
    "            dit[titl] = 'Nan'\n",
    "    return dit\n",
    "\n",
    "\n",
    "#extraction_the_data\n",
    "def extraction_of_Data(resume_path,data_in_dic,text):\n",
    "    prev =[]\n",
    "    person =[]\n",
    "    city = []\n",
    "    education = []\n",
    "    skill =[]\n",
    "    doc = nlp(text)\n",
    "\n",
    "\n",
    "#     extraction of Education\n",
    "    edu_attribute = data_in_dic['Education']\n",
    "    education.append(edu_attribute)\n",
    "    \n",
    "    data1 = resumeparse.read_file(resume_path)\n",
    "    #extract the name\n",
    "    name_data = data1['name']\n",
    "    if 'Aspose Pty' == name_data:\n",
    "        people =[]\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\" or (ent.text.istitle()):\n",
    "                people.append(ent.text)\n",
    "        name_data = people[2]\n",
    "    elif 'curriculum vitae' == name_data.lower():\n",
    "        people =[]\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\" or (ent.text.istitle()):\n",
    "                people.append(ent.text)\n",
    "                break\n",
    "        name_data = people[0]\n",
    "    else:\n",
    "        name_data = name_data\n",
    "    #extract the email\n",
    "    email_data = data1['email']\n",
    "    #extract the phone\n",
    "    phone = data1['phone']\n",
    "    if str(phone)=='003-2023':\n",
    "        phone_pattern = re.compile(r'(?:\\+\\d{1,2}\\s)?\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}')\n",
    "        phone_numbers = re.findall(phone_pattern, text)\n",
    "        phone =phone_numbers[0]\n",
    "    else:\n",
    "        phone = phone\n",
    "\n",
    "    #extract the designition\n",
    "    designation_data = data1['designition']\n",
    "    \n",
    "    #extract the experience\n",
    "    experience_data = data1['total_exp']\n",
    "    if len([i for i in str(experience_data)])>2:\n",
    "        experience_data = None\n",
    "    else:\n",
    "        experience_data = experience_data\n",
    "        \n",
    "    #extract the skills\n",
    "#     skill_data = data1['skills']\n",
    "        \n",
    "    #extract the New_skills    \n",
    "    skill_attribute = data_in_dic['Skills']\n",
    "    if str(skill_attribute) =='Nan':\n",
    "        skill.append(data1['skills'])\n",
    "    else:\n",
    "        skill_attribute = data_in_dic['Skills']\n",
    "        skill.append(skill_attribute)\n",
    "    \n",
    "    #append the information \n",
    "#     prev.append(person)\n",
    "    prev.append(name_data)\n",
    "    prev.append(email_data)\n",
    "    prev.append(phone)\n",
    "    prev.append(skill)\n",
    "    prev.append(designation_data)\n",
    "    prev.append(experience_data)\n",
    "    prev.append(education)\n",
    "#     prev.append(city)\n",
    "#location\n",
    "    place_entity = locationtagger.find_locations(text = text)\n",
    "    prev.append(str(place_entity.countries))\n",
    "    prev.append(str(place_entity.regions))\n",
    "    prev.append(str(place_entity.cities))\n",
    "    prev.append(resume_path)\n",
    "    return prev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176c6083",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Name', 'Email','Mobile','Skills','Designation','Experience_Period','education','countries','regions','cities','Resume_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f367a104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:'NoneType' object has no attribute 'group'\n",
      "ERROR:root:invalid literal for int() with base 10: 'October2010'\n",
      "ERROR:root:'NoneType' object has no attribute 'group'\n",
      "ERROR:root:invalid literal for int() with base 10: 'feb15'\n",
      "ERROR:root:'NoneType' object has no attribute 'group'\n",
      "ERROR:root:'NoneType' object has no attribute 'group'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input any text or url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:invalid literal for int() with base 10: '2012-14'\n",
      "ERROR:root:invalid literal for int() with base 10: 'July2015'\n",
      "ERROR:root:'NoneType' object has no attribute 'group'\n",
      "ERROR:root:invalid literal for int() with base 10: 'till Date'\n",
      "ERROR:root:'NoneType' object has no attribute 'group'\n",
      "ERROR:root:invalid literal for int() with base 10: 'july2018'\n",
      "ERROR:root:invalid literal for int() with base 10: 'date'\n",
      "ERROR:root:'NoneType' object has no attribute 'group'\n",
      "ERROR:root:invalid literal for int() with base 10: 'March2014'\n",
      "ERROR:root:invalid literal for int() with base 10: '10:19'\n",
      "ERROR:root:'NoneType' object has no attribute 'group'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input any text or url\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:'NoneType' object has no attribute 'group'\n",
      "ERROR:root:'NoneType' object has no attribute 'group'\n",
      "ERROR:root:invalid literal for int() with base 10: '25.10.14'\n",
      "ERROR:root:invalid literal for int() with base 10: '03-july-2011'\n",
      "ERROR:root:invalid literal for int() with base 10: 'july2018'\n",
      "ERROR:root:invalid literal for int() with base 10: 'date'\n",
      "ERROR:root:'NoneType' object has no attribute 'group'\n",
      "ERROR:root:invalid literal for int() with base 10: 'Feb.2016'\n",
      "ERROR:root:invalid literal for int() with base 10: '08 12'\n",
      "ERROR:root:invalid literal for int() with base 10: 'Feb.2012'\n",
      "ERROR:root:invalid literal for int() with base 10: 'Till date'\n",
      "ERROR:root:'NoneType' object has no attribute 'group'\n",
      "ERROR:root:invalid literal for int() with base 10: 'July 2013'\n",
      "ERROR:root:invalid literal for int() with base 10: 'Jan-06'\n",
      "ERROR:root:invalid literal for int() with base 10: '02-16'\n",
      "ERROR:root:invalid literal for int() with base 10: '11-oct-2014'\n",
      "ERROR:root:invalid literal for int() with base 10: 'April 2010'\n",
      "ERROR:root:invalid literal for int() with base 10: '15 November 2014'\n",
      "ERROR:root:invalid literal for int() with base 10: 'July2017'\n",
      "ERROR:root:invalid literal for int() with base 10: '03-july-2011'\n",
      "ERROR:root:invalid literal for int() with base 10: '23-nov-2012'\n",
      "ERROR:root:invalid literal for int() with base 10: 'till date'\n",
      "ERROR:root:'NoneType' object has no attribute 'group'\n",
      "ERROR:root:'NoneType' object has no attribute 'group'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please input any text or url\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc_convert_to_pdf(pdf_file_path)\n",
    "\n",
    "for path_of_list in glob.glob(pdf_file_path):\n",
    "    \n",
    "    if path_of_list.endswith(\".pdf\"):\n",
    "        try:\n",
    "            text = return_text(path_of_list)\n",
    "            data_in_dic = convert_the_info_dic(text)\n",
    "            data = extraction_of_Data(path_of_list,data_in_dic, text)\n",
    "    ########################creating the dataframe###############################\n",
    "            df.loc[-1] = data\n",
    "            df.reset_index(drop ='index',inplace =True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    elif path_of_list.endswith(\".docx\"):\n",
    "        try:\n",
    "            text = return_text(path_of_list)\n",
    "            data_in_dic = convert_the_info_dic(text)\n",
    "            data = extraction_of_Data(path_of_list,data_in_dic, text)\n",
    "    ########################creating the dataframe###############################\n",
    "            df.loc[-1] = data\n",
    "            df.reset_index(drop ='index',inplace =True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07088a12",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af09d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(base_dir,'data_csv','client.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d2516",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df['Skills']:\n",
    "    print(i,'gggggggggggggggggg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0312e3",
   "metadata": {},
   "source": [
    "# data-Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a56df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cities(x):\n",
    "#     x =x[1:-1]\n",
    "#     doc = nlp(x)\n",
    "#     citi=[]\n",
    "#     for ent in doc.ents:\n",
    "#         if ent.label_ == 'GPE'or ent.label_ =='NORP' or ent.label_ == 'FAC':\n",
    "#             citi.append(ent.text)\n",
    "#     return citi\n",
    "\n",
    "# df['cities'] = df['cities'].apply(lambda x :cities(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977ecd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('resume_extration_2_5000.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdb045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv('resume_extration_2_5000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1b189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257cca8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##education_____________\n",
    "\n",
    "# ad = r'Created\\s+with\\s+an\\s+evaluation\\s+copy\\s+of\\s+Aspose\\.Words\\.\\s+To\\s+discover\\s+the\\s+full\\s+versions\\s+of\\s+our\\s+APIs\\s+please\\s+visit:\\s+https://products\\.aspose\\.com/words/'\n",
    "# df['New_Skill'] =df['New_Skill'].apply(lambda x : re.sub(ad, \"\", x))\n",
    "# a,b,c,d,e= '\\\\x0c','\\\\xa0','\\\\u200b','Education','EDUCATION'\n",
    "# dat = [i.replace(a, '').replace(b,'').replace(c,'').replace(d,'').replace(e,'') for i in df['New_Skill'].values]\n",
    "# df['New_Skill'] = dat\n",
    "# df['New_Skill'].replace('\\t',\"\")\n",
    "# df['New_Skill'].replace('\\n',\"\")\n",
    "# df['New_Skill'].replace('\\n\\n',\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740006d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "df['Skills'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", r'\\r+|\\n+|\\t+',r\"\\t|\\n|\\r\"], value=[\"\",\"\",\"\"], regex=True, inplace=True)\n",
    "df.Skills = df.Skills.apply(lambda x: re.sub(r'@[\\S]+', '', str(x)))\n",
    "df.Skills = df.Skills.apply(lambda x: re.sub(r'&[\\S]+?;', '', str(x)))\n",
    "df.Skills = df.Skills.apply(lambda x: re.sub(r'#', ' ', str(x)))\n",
    "df.Skills = df.Skills.apply(lambda x: re.sub(r'(\\bRT\\b|\\bQT\\b)', '', str(x)))\n",
    "df.Skills = df.Skills.apply(lambda x: re.sub(r'http[\\S]+', '', str(x)))\n",
    "df.Skills = df.Skills.apply(lambda x: re.sub(r'\\w*\\d\\w*', r'', str(x)))\n",
    "df.Skills = df.Skills.apply(lambda x: re.sub(r'\\s\\s+', ' ', str(x)))\n",
    "df.Skills = df.Skills.apply(lambda x: re.sub(r'(\\A\\s+|\\s+\\Z)', '', str(x)))\n",
    "df['Skills'].replace('\\t',\"\").replace('\\n',\"\").replace('\\n\\n',\"\").replace(\"\\\\\",\"\")\n",
    "df.Skills.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d08de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "df['New_Skill']= df['New_Skill'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3782b2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    text = re.split('\\W+', str(text))\n",
    "    return text\n",
    "\n",
    "df.New_Skill = df['New_Skill'].apply(lambda x: tokenization(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d200d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    treebank_tag =str(treebank_tag)\n",
    "    if treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatzer(text):\n",
    "    words_and_tags = nltk.pos_tag(text)\n",
    "    lem = []\n",
    "    for word, tag in words_and_tags:\n",
    "        lemma = wn.lemmatize(word,pos =get_wordnet_pos(tag))\n",
    "        lem.append(lemma)\n",
    "    return lem\n",
    "\n",
    "\n",
    "df.New_Skill= df.New_Skill.apply(lambda x: lemmatzer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec11725",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.New_Skill:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6497a334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
