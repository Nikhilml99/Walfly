{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de708cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/anush/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/anush/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/anush/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/anush/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import string\n",
    "from io import BytesIO\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from flask import Flask, render_template, request, url_for, flash, redirect\n",
    "import os\n",
    "import pathlib\n",
    "import string\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8f9768f",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "base_dir = pathlib.Path(__name__).parent.absolute()\n",
    "resume_df = pd.read_csv(os.path.join(base_dir,'data_csv', 'client.csv'))\n",
    "jobs_df = pd.read_csv(os.path.join(base_dir,'data_csv', 'naukari.csv'))\n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56b003f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Preprocessing\n",
    "def data_list(x):\n",
    "    try:\n",
    "        return 'Nan' if x == '[]' else ','.join(x[1:-1].split(','))\n",
    "    except:\n",
    "        return 'Nan'\n",
    "\n",
    "##education_____________\n",
    "def education_email(resume_df):\n",
    "    resume_df[\"Name\"]=resume_df['Name'].apply(lambda x: str(x).lower().replace('CURRICULUM VITAE'.lower(),'Nan'))\n",
    "    resume_df['education'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", r'\\r+|\\n+|\\t+',r\"\\t|\\n|\\r\"], value=[\"\",\"\",\"\"], regex=True, inplace=True)\n",
    "    ad = r'Created\\s+with\\s+an\\s+evaluation\\s+copy\\s+of\\s+Aspose\\.Words\\.\\s+To\\s+discover\\s+the\\s+full\\s+versions\\s+of\\s+our\\s+APIs\\s+please\\s+visit:\\s+https://products\\.aspose\\.com/words/'\n",
    "    resume_df['education'] =resume_df['education'].apply(lambda x : re.sub(ad, \"\", x))\n",
    "    a,b,c,d,e= '\\\\x0c','\\\\xa0','\\\\u200b','Education','EDUCATION'\n",
    "    dat = [i.replace(a, '').replace(b,'').replace(c,'').replace(d,'').replace(e,'') for i in resume_df['education'].values]\n",
    "    resume_df['education'] = dat\n",
    "\n",
    "    #email\n",
    "    dat = [ str(x).lower().replace('e-mail:-','').replace('e-mail :','').replace('email :-','').replace('email:','').replace('email :','').replace('e_mail :-','').replace('e_mail :','').replace('id:','').replace('mail :-','') for x in  resume_df['Email'].values]\n",
    "    resume_df['Email'] =dat\n",
    "    return resume_df\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "# remove_Punctuation___________\n",
    "def remove_punct(text):\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', str(text))\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    treebank_tag = str(treebank_tag)\n",
    "    if treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "\n",
    "def lemmatzer(text):\n",
    "    words_and_tags = nltk.pos_tag(text)\n",
    "    lem = []\n",
    "    for word, tag in words_and_tags:\n",
    "        lemma = wn.lemmatize(word, pos=get_wordnet_pos(tag))\n",
    "        lem.append(lemma)\n",
    "    return lem\n",
    "\n",
    "########################################################################################\n",
    "\n",
    "def resume_merge(resume_df):\n",
    "    resume_df['Experience_Period']= resume_df['Experience_Period'].apply(lambda x: str(x)+' years')\n",
    "    cols = ['Skills', 'education', 'Experience_Period','Designation']\n",
    "    resume_df['Resume'] = resume_df[cols].astype(str).apply(lambda row: '_'.join(row.values.astype(object)), axis=1)\n",
    "    resume_df['Resume'].iloc[0].replace(\"'\",\"\").replace(\"[\",'').replace(\"]\",'')\n",
    "    resume_text = resume_df['Resume']\n",
    "    return resume_text\n",
    "\n",
    "\n",
    "def jobs_df_merge(jobs_df):\n",
    "    jobs_df['Key Skills'] =jobs_df['Key Skills'].apply(lambda x: str(x))\n",
    "    jobs_df['Job Title'] =jobs_df['Job Title'].apply(lambda x: str(x))\n",
    "    jobs_df['Role Category'] =jobs_df['Role Category'].apply(lambda x: str(x))\n",
    "    job_text = jobs_df[['Job Title', 'Job Experience Required', 'Key Skills']].apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "    return job_text\n",
    "job_text = jobs_df_merge(jobs_df)\n",
    "###########################################################################################\n",
    "\n",
    "# for candidate recommendation\n",
    "def cr(resume_df_input,job_text):\n",
    "    resumeprocessed = resume_df_input.copy()\n",
    "    lst = ['Designation', 'Skills', 'education', 'cities', 'countries', 'regions']\n",
    "    for i in lst:\n",
    "        resumeprocessed[f'{i}'] = resumeprocessed[f'{i}'].apply(lambda x: data_list(x))\n",
    "    resumeprocessed = education_email(resumeprocessed)\n",
    "    \n",
    "    for i in lst:\n",
    "        resumeprocessed[f'{i}'] = resumeprocessed[f'{i}'].apply(lambda x: remove_punct(x)).apply(lambda x: tokenization(x)).apply(\n",
    "            lambda x: remove_stopwords(x)).apply(lambda x: lemmatzer(x))\n",
    "    resume_text = resume_merge(resumeprocessed)\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), min_df=0, stop_words='english')\n",
    "    job_features = vectorizer.fit_transform([job_text])\n",
    "    resume_features = vectorizer.transform(resume_text)\n",
    "    # Compute the cosine similarity between the resume and job data\n",
    "    \n",
    "    top_candidate_indices = cosine_similarity(job_features, resume_features)\n",
    "    pred_arr = sorted(top_candidate_indices[0], reverse=True)\n",
    "    pred = sorted(top_candidate_indices[0], reverse=False)\n",
    "    top_candidate_indices=top_candidate_indices.argsort()[0][:3][::-1]\n",
    "    candidate = resume_df.iloc[top_candidate_indices][['Name','Mobile','Email']]\n",
    "    return candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5de2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_text = 'data science, nlp , finetuning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e88567c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time.................. 17.8402361869812\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "ad = cr(resume_df,job_text)\n",
    "ad\n",
    "end = time.time()\n",
    "print('time..................',end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23d23e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d2135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
