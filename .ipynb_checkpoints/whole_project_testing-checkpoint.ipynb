{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05736bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anush/anaconda3/lib/python3.9/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_training' (0.0.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.5). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_286424/3620044724.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mresume_parser\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresumeparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtika\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlocationtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mne_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/resume_parser/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mresume_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresumeparse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresumeparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m __all__ = [\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m'resumeparse'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m ]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/resume_parser/resumeparse.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mskill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mskillsmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mpatterns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskill\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mskillsmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/resume_parser/resumeparse.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mskill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mskillsmatcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhraseMatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m \u001b[0mpatterns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskill\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0mskillsmatcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Job title\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m             )\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_format_docs_and_golds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mvocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab._new_lexeme\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/spacy/lang/en/lex_attrs.py\u001b[0m in \u001b[0;36mlike_num\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdigit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from resume_parser import resumeparse\n",
    "from tika import parser  \n",
    "import locationtagger\n",
    "import re\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from pdfminer.high_level import extract_text\n",
    "import aspose.words as aw\n",
    "import pandas as pd\n",
    "import docx2txt\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "####################################################\n",
    "import os\n",
    "import pathlib\n",
    "import glob\n",
    "from pdfminer.high_level import extract_text\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import docx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c724ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = pathlib.Path(__name__).parent.absolute()\n",
    "pdf_folder_path = os.path.join(base_dir,'Training_resume',)\n",
    "pdf_file_path  = os.path.join(base_dir,'Training_resume','*')\n",
    "pdf_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3639682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_convert_to_pdf(path):\n",
    "#     count =0\n",
    "    cunt =0\n",
    "    cnt = 0\n",
    "    for file_ in glob.glob(path):\n",
    "        try:\n",
    "            if file_.endswith(\".doc\"):\n",
    "                print(file_,'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa')\n",
    "                doc = aw.Document(file_)\n",
    "                doc.save(os.path.join(base_dir,'Training_resume',f\"+{cunt}.pdf\"))\n",
    "                cunt+=1\n",
    "            elif file_.endswith(\".rtf\"):\n",
    "                print(file_,'bbbbbbbbbbbbbbbbbbbbbbbbbbbbbb')\n",
    "                doc = aw.Document(file_)\n",
    "                doc.save(os.path.join(base_dir,'Training_resume',f\"++{cunt}.pdf\"))\n",
    "                cnt+=1        \n",
    "        except  Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "#convert word to text\n",
    "def  extract_text_from_word(word_path):\n",
    "    doc = docx.Document(word_path)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "\n",
    "#convert pdf to text\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path)\n",
    "\n",
    "\n",
    "#return text\n",
    "def return_text(path):\n",
    "    if path.endswith('.docx'):\n",
    "        return extract_text_from_word(path)\n",
    "    elif path.endswith('.pdf'):\n",
    "        return extract_text_from_pdf(path)\n",
    "    \n",
    "\n",
    "#convert_the_info_dic\n",
    "def convert_the_info_dic(text):\n",
    "    #breaking the text\n",
    "    dit ={}\n",
    "    title = ['Education','Qualification','Skills','Objective','Experience','Tools','Designation','Employment']\n",
    "    for i in title:\n",
    "        text = text.replace(f'{i}',f'|{i}').replace(f'{i.upper()}',f'|{i.upper()}')\n",
    "    text = text.split(\"|\")\n",
    "\n",
    "    #making dictionary of inforamtion\n",
    "    for titl in title:\n",
    "        check = False\n",
    "        for txt in text: \n",
    "            if titl in txt or titl.upper() in txt:\n",
    "                check = True\n",
    "                dit[titl] = txt \n",
    "        if check == False:\n",
    "            dit[titl] = 'Nan'\n",
    "    return dit\n",
    "\n",
    "\n",
    "#extraction_the_data\n",
    "def extraction_of_Data(resume_path,data_in_dic,text):\n",
    "    prev =[]\n",
    "    person =[]\n",
    "    city = []\n",
    "    education = []\n",
    "    skill =[]\n",
    "    doc = nlp(text)\n",
    "\n",
    "\n",
    "#     extraction of Education\n",
    "    edu_attribute = data_in_dic['Education']\n",
    "    education.append(edu_attribute)\n",
    "    \n",
    "    data1 = resumeparse.read_file(resume_path)\n",
    "    #extract the name\n",
    "    name_data = data1['name']\n",
    "    if 'Aspose Pty' == name_data:\n",
    "        people =[]\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\" or (ent.text.istitle()):\n",
    "                people.append(ent.text)\n",
    "        name_data = people[2]\n",
    "    elif 'curriculum vitae' == name_data.lower():\n",
    "        people =[]\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == \"PERSON\" or (ent.text.istitle()):\n",
    "                people.append(ent.text)\n",
    "                break\n",
    "        name_data = people[0]\n",
    "    else:\n",
    "        name_data = name_data\n",
    "    #extract the email\n",
    "    email_data = data1['email']\n",
    "    #extract the phone\n",
    "    phone = data1['phone']\n",
    "    if str(phone)=='003-2023':\n",
    "        phone_pattern = re.compile(r'(?:\\+\\d{1,2}\\s)?\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}')\n",
    "        phone_numbers = re.findall(phone_pattern, text)\n",
    "        phone =phone_numbers[0]\n",
    "    else:\n",
    "        phone = phone\n",
    "\n",
    "    #extract the designition\n",
    "    designation_data = data1['designition']\n",
    "    \n",
    "    #extract the experience\n",
    "    experience_data = data1['total_exp']\n",
    "    if len([i for i in str(experience_data)])>2:\n",
    "        experience_data = None\n",
    "    else:\n",
    "        experience_data = experience_data\n",
    "        \n",
    "    #extract the skills\n",
    "#     skill_data = data1['skills']\n",
    "        \n",
    "    #extract the New_skills    \n",
    "    skill_attribute = data_in_dic['Skills']\n",
    "    if str(skill_attribute) =='Nan':\n",
    "        skill.append(data1['skills'])\n",
    "    else:\n",
    "        skill_attribute = data_in_dic['Skills']\n",
    "        skill.append(skill_attribute)\n",
    "    \n",
    "    #append the information \n",
    "#     prev.append(person)\n",
    "    prev.append(name_data)\n",
    "    prev.append(email_data)\n",
    "    prev.append(phone)\n",
    "    prev.append(skill)\n",
    "    prev.append(designation_data)\n",
    "    prev.append(experience_data)\n",
    "    prev.append(education)\n",
    "#     prev.append(city)\n",
    "#location\n",
    "    place_entity = locationtagger.find_locations(text = text)\n",
    "    prev.append(str(place_entity.countries))\n",
    "    prev.append(str(place_entity.regions))\n",
    "    prev.append(str(place_entity.cities))\n",
    "    prev.append(resume_path)\n",
    "    return prev "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab2bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['Name', 'Email','Mobile','Skills','Designation','Experience_Period','education','countries','regions','cities','Resume_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e73e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_file(path_of_list):\n",
    "    try:\n",
    "        text = return_text(path_of_list)\n",
    "        data_in_dic = convert_the_info_dic(text)\n",
    "        data = extraction_of_Data(path_of_list,data_in_dic, text)\n",
    "########################creating the dataframe###############################\n",
    "        df.loc[-1] = data\n",
    "        df.reset_index(drop ='index',inplace =True)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "doc_convert_to_pdf(pdf_file_path)\n",
    "\n",
    "for path_of_list in glob.glob(pdf_file_path):\n",
    "    if path_of_list.endswith(\".pdf\"):\n",
    "        run_file(path_of_list)\n",
    "    \n",
    "    elif path_of_list.endswith(\".docx\"):\n",
    "        run_file(path_of_list)\n",
    "\n",
    "df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664334d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cities(x):\n",
    "    x =x[1:-1]\n",
    "    doc = nlp(x)\n",
    "    citi=[]\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'GPE'or ent.label_ =='NORP' or ent.label_ == 'FAC':\n",
    "            citi.append(ent.text)\n",
    "    return citi\n",
    "\n",
    "df['cities'] = df['cities'].apply(lambda x :cities(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54152de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5755c78",
   "metadata": {},
   "source": [
    "# data_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86669689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# Load the resume and job data\n",
    "\n",
    "resume_df =  pd.read_csv(os.path.join(base_dir,'data_csv','client22.csv'))\n",
    "jobs_df = pd.read_csv(os.path.join(base_dir,'data_csv','naukari.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e85bddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_df.drop('Unnamed: 0',axis=1,inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf45d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = resume_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c83868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name______________________________________________________\n",
    "resume_df[\"Name\"]=resume_df['Name'].apply(lambda x: str(x).lower().replace('CURRICULUM VITAE'.lower(),'Nan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1beefc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_list(x):\n",
    "    try:\n",
    "        return   'Nan' if x  == '[]' else ','.join(x[1:-1].split(','))\n",
    "    except:\n",
    "        return 'Nan'\n",
    "resume_df['Designation']= resume_df['Designation'].apply(lambda x:data_list(x))\n",
    "resume_df['Skills']= resume_df['Skills'].apply(lambda x:data_list(x))\n",
    "resume_df['education']= resume_df['education'].apply(lambda x:data_list(x))\n",
    "resume_df['cities']= resume_df['cities'].apply(lambda x:data_list(x))\n",
    "resume_df['countries'] = resume_df['countries'].apply(lambda x:data_list(x))\n",
    "resume_df['regions'] = resume_df['regions'].apply(lambda x:data_list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4df85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##education_____________\n",
    "import re\n",
    "resume_df['education'].replace(to_replace=[r\"\\\\t|\\\\n|\\\\r\", r'\\r+|\\n+|\\t+',r\"\\t|\\n|\\r\"], value=[\"\",\"\",\"\"], regex=True, inplace=True)\n",
    "ad = r'Created\\s+with\\s+an\\s+evaluation\\s+copy\\s+of\\s+Aspose\\.Words\\.\\s+To\\s+discover\\s+the\\s+full\\s+versions\\s+of\\s+our\\s+APIs\\s+please\\s+visit:\\s+https://products\\.aspose\\.com/words/'\n",
    "resume_df['education'] =resume_df['education'].apply(lambda x : re.sub(ad, \"\", x))\n",
    "a,b,c,d,e= '\\\\x0c','\\\\xa0','\\\\u200b','Education','EDUCATION'\n",
    "dat = [i.replace(a, '').replace(b,'').replace(c,'').replace(d,'').replace(e,'') for i in resume_df['education'].values]\n",
    "resume_df['education'] = dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72908f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "##email_________________________\n",
    "\n",
    "dat = [ str(x).lower().replace('e-mail:-','').replace('e-mail :','').replace('email :-','').replace('email:','').replace('email :','').replace('e_mail :-','').replace('e_mail :','').replace('id:','').replace('mail :-','') for x in  resume_df['Email'].values]\n",
    "resume_df['Email'] =dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70411565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#remove_Punctuation___________\n",
    "\n",
    "import string\n",
    "string.punctuation\n",
    "def remove_punct(text):\n",
    "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return text\n",
    "\n",
    "resume_df['Skills']= resume_df['Skills'].apply(lambda x: remove_punct(x))\n",
    "resume_df['Designation']= resume_df['Designation'].apply(lambda x: remove_punct(x))\n",
    "resume_df['education']= resume_df['education'].apply(lambda x: remove_punct(x))\n",
    "resume_df['countries']= resume_df['countries'].apply(lambda x: remove_punct(x))\n",
    "resume_df['regions']= resume_df['regions'].apply(lambda x: remove_punct(x))\n",
    "resume_df['cities']= resume_df['cities'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "resume_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2073e",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = resume_df.iloc[0].values[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08b0814",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = ' '.join(s).replace(\"SKILLS \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ff869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Skills: nnSoftware Microsoft Powerpoint Outlook Excel Microsoft Word Typing wpmunOperate Avaya phone system nnAWARDSHONORS AND CERTIFICATIONS nnHonor Roll Student nnTeacher certification nnxc ,job description: required python engineer for backend development,Query:How much percentage is Skills is suitable for given job description,Percentage:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7195fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a694a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    text = re.split('\\W+', str(text))\n",
    "    return text\n",
    "\n",
    "resume_df['Skills']= resume_df['Skills'].apply(lambda x: tokenization(x))\n",
    "resume_df['Designation']= resume_df['Designation'].apply(lambda x: tokenization(x))\n",
    "resume_df['education']= resume_df['education'].apply(lambda x: tokenization(x))\n",
    "resume_df['countries']= resume_df['countries'].apply(lambda x: tokenization(x))\n",
    "resume_df['regions']= resume_df['regions'].apply(lambda x: tokenization(x))\n",
    "resume_df['cities']= resume_df['cities'].apply(lambda x: tokenization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this project firstly we extract the data from the resume.\n",
    "# 4\n",
    "# Then we recommended using content based filtering recommnedation engine\n",
    "# 5\n",
    "# For that we used cosine similarity for recommendation system.\n",
    "# 6\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "\n",
    "\n",
    "resume_df['Skills']= resume_df['Skills'].apply(lambda x: remove_stopwords(x))\n",
    "resume_df['Designation']= resume_df['Designation'].apply(lambda x: remove_stopwords(x))\n",
    "resume_df['education']= resume_df['education'].apply(lambda x: remove_stopwords(x))\n",
    "resume_df['countries']= resume_df['countries'].apply(lambda x: remove_stopwords(x))\n",
    "resume_df['regions']= resume_df['regions'].apply(lambda x: remove_stopwords(x))\n",
    "resume_df['cities']= resume_df['cities'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9261554e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.corpus import wordnet\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "wn = nltk.WordNetLemmatizer()\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    treebank_tag =str(treebank_tag)\n",
    "    if treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "def lemmatzer(text):\n",
    "    words_and_tags = nltk.pos_tag(text)\n",
    "    lem = []\n",
    "    for word, tag in words_and_tags:\n",
    "        lemma = wn.lemmatize(word,pos =get_wordnet_pos(tag))\n",
    "        lem.append(lemma)\n",
    "    return lem\n",
    "\n",
    "# resume_df['New_Skill']= resume_df['New_Skill'].apply(lambda x: lemmatzer(x))\n",
    "resume_df['Skills']= resume_df['Skills'].apply(lambda x: lemmatzer(x))\n",
    "resume_df['Designation']= resume_df['Designation'].apply(lambda x: lemmatzer(x))\n",
    "resume_df['education']= resume_df['education'].apply(lambda x: lemmatzer(x))\n",
    "resume_df['countries']= resume_df['countries'].apply(lambda x: lemmatzer(x))\n",
    "resume_df['regions']= resume_df['regions'].apply(lambda x: lemmatzer(x))\n",
    "resume_df['cities']= resume_df['cities'].apply(lambda x: lemmatzer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(resume_df['Skills'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5d8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.DataFrame(columns =['Skills','Designation','education','countries','regions','cities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ec0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_into_string(x):\n",
    "    return ''.join(x)\n",
    "dat['Skills']= resume_df['Skills'].apply(lambda x: ' '.join(x))\n",
    "dat['Designation']= resume_df['Designation'].apply(lambda x: ' '.join(x))\n",
    "dat['education']= resume_df['education'].apply(lambda x: ' '.join(x))\n",
    "dat['countries']= resume_df['countries'].apply(lambda x: ' '.join(x))\n",
    "dat['regions']= resume_df['regions'].apply(lambda x: ' '.join(x))\n",
    "dat['cities']= resume_df['cities'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f47006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "lst = ['Skills','Designation','education','countries','regions','cities']\n",
    "\n",
    "def word_cloud(dat,lst):\n",
    "    plt.figure(figsize=(6,6))\n",
    "    count =1\n",
    "    for col in lst:\n",
    "        plt.subplot(3,2, count)\n",
    "        wc = WordCloud(background_color=\"black\", stopwords=STOPWORDS,width=250, height=180)\n",
    "        wc.generate(\" \".join(dat[str(col)]))\n",
    "        plt.imshow(wc.recolor( colormap= 'Pastel2' , random_state=17), alpha=0.98)\n",
    "        plt.title(str(col), fontsize=10)\n",
    "        plt.axis('off')\n",
    "        count+=1\n",
    "\n",
    "word_cloud(dat,lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af81e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a201b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_df['Experience_Period']= resume_df['Experience_Period'].apply(lambda x: str(x)+' years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fd27b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Skills', 'education', 'Experience_Period','Designation']\n",
    "resume_df['Resume'] = resume_df[cols].astype(str).apply(lambda row: '_'.join(row.values.astype(object)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b87bf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resume_df['Resume'].iloc[0].replace(\"'\",\"\").replace(\"[\",'').replace(\"]\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5fc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b039146",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_text = jobs_df[['Job Title', 'Job Experience Required', 'Key Skills']].apply(lambda x: ' '.join(x.astype(str)), axis=1)\n",
    "vectorizer = TfidfVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9717c4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def givejob(resume_text,vectorizer):\n",
    "    resume_features = vectorizer.fit_transform([resume_text]) \n",
    "    print(resume_features.shape)\n",
    "    job_features = vectorizer.transform(job_text) \n",
    "    print(job_features.shape)\n",
    "    # Compute the cosine similarity between the resume and job data\n",
    "    top_job_indices = cosine_similarity(resume_features, job_features)\n",
    "    prd_arr = sorted(top_job_indices[0],reverse = True)[:5]\n",
    "    top_job_indices=top_job_indices.argsort()[0][::-1][:5]\n",
    "    jobs = jobs_df.iloc[top_job_indices]['Job Title'].tolist()\n",
    "    return jobs,prd_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b3e3b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "jobs, prd_arr = givejob(resume_df['Resume'][1],vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a3a0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prd_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f87952",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a1afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "courses = jobs\n",
    "values = prd_arr\n",
    "\n",
    "fig = plt.figure(figsize = (25, 15))\n",
    " \n",
    "# creating the bar plot\n",
    "plt.bar(courses, values, color =['lightgreen','cyan','lightblue','green','red'],width = 0.4,edgecolor='blue',)\n",
    "plt.xlabel(\"recomendate job\",fontsize=20)\n",
    "plt.ylabel(\"similarity in percentage\",fontsize=20)\n",
    "plt.title(\"jobs recomendation\",fontsize = 20)\n",
    "plt.legend(loc='best', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471264cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f58df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
